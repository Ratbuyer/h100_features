//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_90a
.address_size 64

	// .globl	_Z14cuda_core_workPi
// _ZZ16tensor_core_workPiE8A_shared has been demoted
// _ZZ16tensor_core_workPiE8B_shared has been demoted
// _ZZ10overlap_v2PiE8A_shared has been demoted
// _ZZ10overlap_v2PiE8B_shared has been demoted
// _ZZ10overlap_v1PiE8A_shared has been demoted
// _ZZ10overlap_v1PiE8B_shared has been demoted
// _ZZ10overlap_v3PiE8A_shared has been demoted
// _ZZ10overlap_v3PiE8B_shared has been demoted

.visible .entry _Z14cuda_core_workPi(
	.param .u64 _Z14cuda_core_workPi_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [_Z14cuda_core_workPi_param_0];
	mov.f32 	%f27, 0f00000000;
	mov.u32 	%r5, 0;
	bra.uni 	$L__BB0_1;

$L__BB0_2:
	fma.rn.f32 	%f21, %f5, %f5, %f2;
	fma.rn.f32 	%f23, %f7, %f7, %f21;
	fma.rn.f32 	%f25, %f9, %f9, %f23;
	fma.rn.f32 	%f27, %f11, %f11, %f25;
	add.s32 	%r5, %r5, 4;

$L__BB0_1:
	mov.f32 	%f5, 0f3F800000;
	fma.rn.f32 	%f6, %f5, %f5, %f27;
	mov.f32 	%f7, 0f3F8CCCCD;
	fma.rn.f32 	%f8, %f7, %f7, %f6;
	mov.f32 	%f9, 0f3F99999A;
	fma.rn.f32 	%f10, %f9, %f9, %f8;
	mov.f32 	%f11, 0f3FA66666;
	fma.rn.f32 	%f12, %f11, %f11, %f10;
	fma.rn.f32 	%f13, %f5, %f5, %f12;
	fma.rn.f32 	%f14, %f7, %f7, %f13;
	fma.rn.f32 	%f15, %f9, %f9, %f14;
	fma.rn.f32 	%f16, %f11, %f11, %f15;
	fma.rn.f32 	%f17, %f5, %f5, %f16;
	fma.rn.f32 	%f18, %f7, %f7, %f17;
	fma.rn.f32 	%f19, %f9, %f9, %f18;
	fma.rn.f32 	%f2, %f11, %f11, %f19;
	setp.eq.s32 	%p1, %r5, 9999996;
	@%p1 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_2;

$L__BB0_3:
	cvta.to.global.u64 	%rd2, %rd1;
	cvt.rzi.s32.f32 	%r4, %f2;
	st.global.u32 	[%rd2], %r4;
	ret;

}
	// .globl	_Z16tensor_core_workPi
.visible .entry _Z16tensor_core_workPi(
	.param .u64 _Z16tensor_core_workPi_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 16 .b8 _ZZ16tensor_core_workPiE8A_shared[2048];
	// demoted variable
	.shared .align 16 .b8 _ZZ16tensor_core_workPiE8B_shared[256];

	ld.param.u64 	%rd1, [_Z16tensor_core_workPi_param_0];
	// begin inline asm
	wgmma.fence.sync.aligned; 

	// end inline asm
	mov.u32 	%r37, 0;
	mov.u32 	%r24, _ZZ16tensor_core_workPiE8A_shared;
	shr.u32 	%r25, %r24, 4;
	cvt.u64.u32 	%rd8, %r25;
	and.b64  	%rd9, %rd8, 16383;
	or.b64  	%rd6, %rd9, 68720001024;
	mov.u32 	%r26, _ZZ16tensor_core_workPiE8B_shared;
	shr.u32 	%r27, %r26, 4;
	cvt.u64.u32 	%rd10, %r27;
	and.b64  	%rd11, %rd10, 16383;
	or.b64  	%rd7, %rd11, 68720001024;
	mov.u32 	%r38, %r37;
	mov.u32 	%r39, %r37;
	bra.uni 	$L__BB1_1;

$L__BB1_2:
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r38, %r39}, %rd6, %rd7, 1, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r37, %r37, 4;

$L__BB1_1:
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r38, %r39}, %rd6, %rd7, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r38, %r39}, %rd6, %rd7, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r38, %r39}, %rd6, %rd7, 1, 1, 1, 0, 0;
	// end inline asm
	setp.eq.s32 	%p1, %r37, 9999996;
	@%p1 bra 	$L__BB1_3;
	bra.uni 	$L__BB1_2;

$L__BB1_3:
	cvta.to.global.u64 	%rd18, %rd1;
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	// begin inline asm
	wgmma.wait_group.sync.aligned 0; 

	// end inline asm
	// begin inline asm
	wgmma.fence.sync.aligned; 

	// end inline asm
	add.s32 	%r36, %r38, %r39;
	st.global.u32 	[%rd18], %r36;
	ret;

}
	// .globl	_Z10overlap_v2Pi
.visible .entry _Z10overlap_v2Pi(
	.param .u64 _Z10overlap_v2Pi_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 16 .b8 _ZZ10overlap_v2PiE8A_shared[2048];
	// demoted variable
	.shared .align 16 .b8 _ZZ10overlap_v2PiE8B_shared[256];

	ld.param.u64 	%rd1, [_Z10overlap_v2Pi_param_0];
	// begin inline asm
	wgmma.fence.sync.aligned; 

	// end inline asm
	mov.u32 	%r39, 0;
	mov.f32 	%f27, 0f00000000;
	mov.u32 	%r24, _ZZ10overlap_v2PiE8A_shared;
	shr.u32 	%r25, %r24, 4;
	cvt.u64.u32 	%rd8, %r25;
	and.b64  	%rd9, %rd8, 16383;
	or.b64  	%rd6, %rd9, 68720001024;
	mov.u32 	%r26, _ZZ10overlap_v2PiE8B_shared;
	shr.u32 	%r27, %r26, 4;
	cvt.u64.u32 	%rd10, %r27;
	and.b64  	%rd11, %rd10, 16383;
	or.b64  	%rd7, %rd11, 68720001024;
	mov.u32 	%r40, %r39;
	mov.u32 	%r41, %r39;
	bra.uni 	$L__BB2_1;

$L__BB2_2:
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r40, %r41}, %rd6, %rd7, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	fma.rn.f32 	%f21, %f5, %f5, %f2;
	fma.rn.f32 	%f23, %f7, %f7, %f21;
	fma.rn.f32 	%f25, %f9, %f9, %f23;
	fma.rn.f32 	%f27, %f11, %f11, %f25;
	add.s32 	%r39, %r39, 4;

$L__BB2_1:
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r40, %r41}, %rd6, %rd7, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	mov.f32 	%f5, 0f3F800000;
	fma.rn.f32 	%f6, %f5, %f5, %f27;
	mov.f32 	%f7, 0f3F8CCCCD;
	fma.rn.f32 	%f8, %f7, %f7, %f6;
	mov.f32 	%f9, 0f3F99999A;
	fma.rn.f32 	%f10, %f9, %f9, %f8;
	mov.f32 	%f11, 0f3FA66666;
	fma.rn.f32 	%f12, %f11, %f11, %f10;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r40, %r41}, %rd6, %rd7, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	fma.rn.f32 	%f13, %f5, %f5, %f12;
	fma.rn.f32 	%f14, %f7, %f7, %f13;
	fma.rn.f32 	%f15, %f9, %f9, %f14;
	fma.rn.f32 	%f16, %f11, %f11, %f15;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r40, %r41}, %rd6, %rd7, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	fma.rn.f32 	%f17, %f5, %f5, %f16;
	fma.rn.f32 	%f18, %f7, %f7, %f17;
	fma.rn.f32 	%f19, %f9, %f9, %f18;
	fma.rn.f32 	%f2, %f11, %f11, %f19;
	setp.eq.s32 	%p1, %r39, 9999996;
	@%p1 bra 	$L__BB2_3;
	bra.uni 	$L__BB2_2;

$L__BB2_3:
	cvta.to.global.u64 	%rd18, %rd1;
	// begin inline asm
	wgmma.wait_group.sync.aligned 0; 

	// end inline asm
	// begin inline asm
	wgmma.fence.sync.aligned; 

	// end inline asm
	add.s32 	%r36, %r40, %r41;
	mov.b32 	%r37, %f2;
	add.s32 	%r38, %r36, %r37;
	st.global.u32 	[%rd18], %r38;
	ret;

}
	// .globl	_Z10overlap_v1Pi
.visible .entry _Z10overlap_v1Pi(
	.param .u64 _Z10overlap_v1Pi_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<19>;
	// demoted variable
	.shared .align 16 .b8 _ZZ10overlap_v1PiE8A_shared[2048];
	// demoted variable
	.shared .align 16 .b8 _ZZ10overlap_v1PiE8B_shared[256];

	ld.param.u64 	%rd2, [_Z10overlap_v1Pi_param_0];
	// begin inline asm
	wgmma.fence.sync.aligned; 

	// end inline asm
	mov.u32 	%r43, 0;
	mov.u32 	%r27, _ZZ10overlap_v1PiE8A_shared;
	shr.u32 	%r28, %r27, 4;
	cvt.u64.u32 	%rd9, %r28;
	and.b64  	%rd10, %rd9, 16383;
	or.b64  	%rd7, %rd10, 68720001024;
	mov.u32 	%r29, _ZZ10overlap_v1PiE8B_shared;
	shr.u32 	%r30, %r29, 4;
	cvt.u64.u32 	%rd11, %r30;
	and.b64  	%rd12, %rd11, 16383;
	or.b64  	%rd8, %rd12, 68720001024;
	mov.u32 	%r44, %r43;
	mov.u32 	%r45, %r43;
	bra.uni 	$L__BB3_1;

$L__BB3_2:
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r44, %r45}, %rd7, %rd8, 1, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r43, %r43, 4;

$L__BB3_1:
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r44, %r45}, %rd7, %rd8, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r44, %r45}, %rd7, %rd8, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r44, %r45}, %rd7, %rd8, 1, 1, 1, 0, 0;
	// end inline asm
	setp.eq.s32 	%p1, %r43, 9999996;
	@%p1 bra 	$L__BB3_3;
	bra.uni 	$L__BB3_2;

$L__BB3_3:
	cvta.to.global.u64 	%rd1, %rd2;
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	mov.f32 	%f27, 0f00000000;
	mov.u32 	%r46, 0;
	bra.uni 	$L__BB3_4;

$L__BB3_5:
	fma.rn.f32 	%f21, %f5, %f5, %f2;
	fma.rn.f32 	%f23, %f7, %f7, %f21;
	fma.rn.f32 	%f25, %f9, %f9, %f23;
	fma.rn.f32 	%f27, %f11, %f11, %f25;
	add.s32 	%r46, %r46, 4;

$L__BB3_4:
	mov.f32 	%f5, 0f3F800000;
	fma.rn.f32 	%f6, %f5, %f5, %f27;
	mov.f32 	%f7, 0f3F8CCCCD;
	fma.rn.f32 	%f8, %f7, %f7, %f6;
	mov.f32 	%f9, 0f3F99999A;
	fma.rn.f32 	%f10, %f9, %f9, %f8;
	mov.f32 	%f11, 0f3FA66666;
	fma.rn.f32 	%f12, %f11, %f11, %f10;
	fma.rn.f32 	%f13, %f5, %f5, %f12;
	fma.rn.f32 	%f14, %f7, %f7, %f13;
	fma.rn.f32 	%f15, %f9, %f9, %f14;
	fma.rn.f32 	%f16, %f11, %f11, %f15;
	fma.rn.f32 	%f17, %f5, %f5, %f16;
	fma.rn.f32 	%f18, %f7, %f7, %f17;
	fma.rn.f32 	%f19, %f9, %f9, %f18;
	fma.rn.f32 	%f2, %f11, %f11, %f19;
	setp.eq.s32 	%p2, %r46, 9999996;
	@%p2 bra 	$L__BB3_6;
	bra.uni 	$L__BB3_5;

$L__BB3_6:
	// begin inline asm
	wgmma.wait_group.sync.aligned 0; 

	// end inline asm
	// begin inline asm
	wgmma.fence.sync.aligned; 

	// end inline asm
	add.s32 	%r40, %r44, %r45;
	mov.b32 	%r41, %f2;
	add.s32 	%r42, %r40, %r41;
	st.global.u32 	[%rd1], %r42;
	ret;

}
	// .globl	_Z10overlap_v3Pi
.visible .entry _Z10overlap_v3Pi(
	.param .u64 _Z10overlap_v3Pi_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<44>;
	.reg .b32 	%r<58>;
	.reg .b64 	%rd<27>;
	// demoted variable
	.shared .align 16 .b8 _ZZ10overlap_v3PiE8A_shared[2048];
	// demoted variable
	.shared .align 16 .b8 _ZZ10overlap_v3PiE8B_shared[256];

	ld.param.u64 	%rd1, [_Z10overlap_v3Pi_param_0];
	// begin inline asm
	wgmma.fence.sync.aligned; 

	// end inline asm
	mov.u32 	%r55, 0;
	mov.f32 	%f43, 0f00000000;
	mov.u32 	%r36, _ZZ10overlap_v3PiE8A_shared;
	shr.u32 	%r37, %r36, 4;
	cvt.u64.u32 	%rd14, %r37;
	and.b64  	%rd15, %rd14, 16383;
	or.b64  	%rd12, %rd15, 68720001024;
	mov.u32 	%r38, _ZZ10overlap_v3PiE8B_shared;
	shr.u32 	%r39, %r38, 4;
	cvt.u64.u32 	%rd16, %r39;
	and.b64  	%rd17, %rd16, 16383;
	or.b64  	%rd13, %rd17, 68720001024;
	mov.u32 	%r56, %r55;
	mov.u32 	%r57, %r55;
	bra.uni 	$L__BB4_1;

$L__BB4_2:
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r56, %r57}, %rd12, %rd13, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r56, %r57}, %rd12, %rd13, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	fma.rn.f32 	%f33, %f5, %f5, %f2;
	fma.rn.f32 	%f35, %f7, %f7, %f33;
	fma.rn.f32 	%f37, %f9, %f9, %f35;
	fma.rn.f32 	%f39, %f11, %f11, %f37;
	fma.rn.f32 	%f40, %f5, %f5, %f39;
	fma.rn.f32 	%f41, %f7, %f7, %f40;
	fma.rn.f32 	%f42, %f9, %f9, %f41;
	fma.rn.f32 	%f43, %f11, %f11, %f42;
	add.s32 	%r55, %r55, 4;

$L__BB4_1:
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r56, %r57}, %rd12, %rd13, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r56, %r57}, %rd12, %rd13, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	mov.f32 	%f5, 0f3F800000;
	fma.rn.f32 	%f6, %f5, %f5, %f43;
	mov.f32 	%f7, 0f3F8CCCCD;
	fma.rn.f32 	%f8, %f7, %f7, %f6;
	mov.f32 	%f9, 0f3F99999A;
	fma.rn.f32 	%f10, %f9, %f9, %f8;
	mov.f32 	%f11, 0f3FA66666;
	fma.rn.f32 	%f12, %f11, %f11, %f10;
	fma.rn.f32 	%f13, %f5, %f5, %f12;
	fma.rn.f32 	%f14, %f7, %f7, %f13;
	fma.rn.f32 	%f15, %f9, %f9, %f14;
	fma.rn.f32 	%f16, %f11, %f11, %f15;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r56, %r57}, %rd12, %rd13, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r56, %r57}, %rd12, %rd13, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	fma.rn.f32 	%f17, %f5, %f5, %f16;
	fma.rn.f32 	%f18, %f7, %f7, %f17;
	fma.rn.f32 	%f19, %f9, %f9, %f18;
	fma.rn.f32 	%f20, %f11, %f11, %f19;
	fma.rn.f32 	%f21, %f5, %f5, %f20;
	fma.rn.f32 	%f22, %f7, %f7, %f21;
	fma.rn.f32 	%f23, %f9, %f9, %f22;
	fma.rn.f32 	%f24, %f11, %f11, %f23;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r56, %r57}, %rd12, %rd13, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 {%r56, %r57}, %rd12, %rd13, 1, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned; 

	// end inline asm
	fma.rn.f32 	%f25, %f5, %f5, %f24;
	fma.rn.f32 	%f26, %f7, %f7, %f25;
	fma.rn.f32 	%f27, %f9, %f9, %f26;
	fma.rn.f32 	%f28, %f11, %f11, %f27;
	fma.rn.f32 	%f29, %f5, %f5, %f28;
	fma.rn.f32 	%f30, %f7, %f7, %f29;
	fma.rn.f32 	%f31, %f9, %f9, %f30;
	fma.rn.f32 	%f2, %f11, %f11, %f31;
	setp.eq.s32 	%p1, %r55, 4999996;
	@%p1 bra 	$L__BB4_3;
	bra.uni 	$L__BB4_2;

$L__BB4_3:
	cvta.to.global.u64 	%rd26, %rd1;
	// begin inline asm
	wgmma.wait_group.sync.aligned 0; 

	// end inline asm
	// begin inline asm
	wgmma.fence.sync.aligned; 

	// end inline asm
	add.s32 	%r52, %r56, %r57;
	mov.b32 	%r53, %f2;
	add.s32 	%r54, %r52, %r53;
	st.global.u32 	[%rd26], %r54;
	ret;

}